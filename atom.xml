<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://yuncheng1998.github.io</id>
    <title>云程的BLOG</title>
    <updated>2021-09-02T02:42:28.317Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://yuncheng1998.github.io"/>
    <link rel="self" href="https://yuncheng1998.github.io/atom.xml"/>
    <subtitle>潜龙勿用</subtitle>
    <logo>https://yuncheng1998.github.io/images/avatar.png</logo>
    <icon>https://yuncheng1998.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, 云程的BLOG</rights>
    <entry>
        <title type="html"><![CDATA[证明该网站为个人网站]]></title>
        <id>https://yuncheng1998.github.io/post/QTTY-I_Cs/</id>
        <link href="https://yuncheng1998.github.io/post/QTTY-I_Cs/">
        </link>
        <updated>2021-09-02T02:40:59.000Z</updated>
        <content type="html"><![CDATA[<p>该网站为我的个人网站，本人的掘金地址为https://juejin.cn/user/2752832849322519，文章为个人网站搬运并非抄袭</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SQL查询语句的执行顺序]]></title>
        <id>https://yuncheng1998.github.io/post/OPUdvhLwj/</id>
        <link href="https://yuncheng1998.github.io/post/OPUdvhLwj/">
        </link>
        <updated>2021-07-11T05:31:05.000Z</updated>
        <content type="html"><![CDATA[<p>SQL queries happen in this order<br>
<img src="https://yuncheng1998.github.io/post-images/1625981788114.jpeg" alt="the order of query" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[延时队列]]></title>
        <id>https://yuncheng1998.github.io/post/U2aPPg3VM/</id>
        <link href="https://yuncheng1998.github.io/post/U2aPPg3VM/">
        </link>
        <updated>2021-06-01T02:22:17.000Z</updated>
        <content type="html"><![CDATA[<p>dockone.io/article/10139</p>
<p>当应用要求操作事件是顺序的，并且事件由同一个终端设备发送，通过设备<em>ID</em>计算<em>Hash</em>到同一个节点服务处理，这之中不存在时钟一致性问题，但由于事件发送是异步的，所以接收可能乱序，再比如在大数据系统中分析<em>OAuth</em>关系，<em>OAuth</em>表记录的是<em>A</em>应用的<em>X</em>用户与<em>B</em>应用的<em>Y</em>用户的关联（如果<em>B</em>应用没有对应的用户则<em>Y</em>用户为新增记录），但用户表、应用表和<em>OAuth</em>表都是分开采集的，即不能保证分析<em>OAuth</em>表时用户表对应的用户就一定已经存在。对于这类需求比较通用的解决方案是使用延迟队列。</p>
<p>什么是延时队列？顾名思义：首先它要具有队列的特性，再给它附加一个延迟消费队列消息的功能，也就是说可以指定队列中的消息在哪个时间点被消费。</p>
<h1 id="delayqueue"><em>DelayQueue</em></h1>
<p><em>DelayQueue</em>是一个<em>BlockingQueue</em>（无界阻塞）队列，它封装了一个使用完全二叉堆排序元素的<em>PriorityQueue</em>（优先队列），在添加元素时，使用<em>Delay</em>（延迟时间）作为排序条件，延迟最小的元素会优先放在队首。我们可以队列中的元素只有到了<em>Delay</em>时间才允许从队列中取出，从而得到延时队列。<br>
<img src="https://yuncheng1998.github.io/post-images/1622514280644.png" alt="" loading="lazy"></p>
<pre><code class="language-java">@Data
public class Order implements Delayed {

  private final long timestamp;

  private final String name;

  public Order(String name, long timestamp, TimeUnit unit) {
    this.timestamp = System.currentTimeMillis() + (timestamp &gt; 0 ? unit.toMillis(timestamp) : 0);
    this.name = name;
  }

  /**
   * 返回剩余的延迟
   * poll方法根据该方法判断，只有延迟&lt;0才出队
   */
  @Override
  public long getDelay(TimeUnit unit) {
    return timestamp - System.currentTimeMillis();
  }

  /**
   * 决定堆排序
   */
  @Override
  public int compareTo(Delayed o) {
    Order Order = (Order) o;
    long diff = this.timestamp - Order.timestamp;
    if (diff &lt;= 0) {
      return -1;
    } else {
      return 1;
    }
  }
}
</code></pre>
<p><em>DelayQueue</em>的<em>put</em>方法内部使用了<em>ReentrantLock</em>锁进行线程同步，因此是线程安全的。<em>DelayQueue</em>还提供了两种出队的方法<em>poll()<em>和</em>take()</em> ， <em>poll()<em>为非阻塞获取，没有到期的元素直接返回</em>null</em>；*take()*阻塞方式获取，没有到期的元素线程将会等待。</p>
<pre><code class="language-java">public static void main(String[] args) throws InterruptedException {
    Order order1 = new Order(&quot;订单1&quot;, 5, TimeUnit.SECONDS);
    Order order2 = new Order(&quot;订单2&quot;, 10, TimeUnit.SECONDS);
    Order order3 = new Order(&quot;订单3&quot;, 15, TimeUnit.SECONDS);
    DelayQueue&lt;Order&gt; delayQueue = new DelayQueue&lt;&gt;();
    delayQueue.put(order1);
    delayQueue.put(order3);
    delayQueue.put(order2);

    System.out.println(&quot;订单延迟队列开始时间:&quot; + new DateTime().toString(&quot;HH:mm:ss&quot;));
    while (delayQueue.size() != 0) {
      // 取队列头部元素是否过期
      Order task = delayQueue.poll();
      if (task != null) {
        System.out.format(&quot;%s被取消, 时间:{%s}\n&quot;, task.getName(), new DateTime().toString(&quot;HH:mm:ss&quot;));
      }
      Thread.sleep(1000);
    }
}
</code></pre>
<p>结果</p>
<blockquote>
<p>订单1被取消, 时间:{15:49:38}<br>
订单2被取消, 时间:{15:49:43}<br>
订单3被取消, 时间:{15:49:49}</p>
</blockquote>
<h1 id="quartz-定时任务"><em>Quartz</em> 定时任务</h1>
<p><em>Quartz</em>一款非常经典任务调度框架，在<em>Redis</em>、<em>RabbitMQ</em>还未广泛应用时，超时未支付取消订单功能都是由定时任务实现的。定时任务它有一定的周期性，可能很多单子已经超时，但还没到达触发执行的时间点，那么就会造成订单处理的不够及时。</p>
<h1 id="redis-zset"><em>Redis Zset</em></h1>
<p>利用<em>Redis</em>的<em>Zset</em>可以实现延迟队列的效果，通过设置<em>Score</em>属性为集合中的成员进行从小到大的排序。</p>
<pre><code class="language-scala">// ----------- 延迟消息写入逻辑 -----------
// 保存消息内容，kind是消息类型，如订单到期、OAuth延迟处理等，id是消息的记录Id
redis.hset(&quot;delay:body:&quot;+timerTaskReq.kind, timerTaskReq.id, toJsonString(timerTaskReq))
// 删除之前的延迟时间（如果存在的话）
redis.zrem(&quot;delay:queue:&quot; + timerTaskReq.kind, timerTaskReq.id)
// 添加新的延迟时间，timerTaskReq.execMs为期望执行（到期）的时间，这里用这个时间做为评分
redis.zadd(&quot;delay:queue:&quot; + timerTaskReq.kind, timerTaskReq.execMs, timerTaskReq.id)
// ----------------------------------------

// -------- 延迟消息获取及发送逻辑 --------
// kinds为所有的消息类型
kinds.map{
    kind -&gt;
    // 获取过期的消息Id，即评分为0到当前时间戳
    var expireTaskIds = redis.zrangebyscore(&quot;delay:queue:&quot; + kind, 0, currentTimeMs)
    // 获取对应的消息内容
    var expireTasks = redis.hmget(&quot;delay:body:&quot; + kind, expireTaskIds)
    // 删除过期的消息Id
    redis.zremMany(&quot;delay:queue:&quot; + kind, expireTaskIds)
    // 删除过期的消息内容
    redis.hdelMany(&quot;delay:body:&quot; + kind, expireTaskIds)
    // 发送消息
    sendTask(expireTasks)
}
</code></pre>
<p>利用<em>Redis</em>的<em>key</em>过期回调事件：开启监听<em>key</em>是否过期的事件，一旦<em>key</em>过期会触发一个<em>callback</em>事件。但不能应用其完成延迟队列，因为<em>Redis</em>只在过期键被删除的时候通知，而不是键的生存时间变为<em>0</em>的时候立马通知。过期键的删除是由一个后台任务执行，为不影响关键业务，后台任务被严格限制，默认为一秒执行<em>10</em>次，一次最多<em>250</em>ms，可通过<em>hz</em>参数调整，但当过期键比例很高时仍然会出现大量的通知的延迟。</p>
<h1 id="rabbitmq-延时队列"><em>RabbitMQ</em> 延时队列</h1>
<p><em>RabbitMQ</em> 存在两个属性： *TTL（Time To Live）*和 <em>DLX（Dead Letter Exchange）</em>。</p>
<p><em>TTL</em>指消息存活的时间，<em>RabbitMQ</em>通过<em>x-message-tt</em>参数来设置指定队列（队列中所有消息都具有相同的过期时间）或消息（某一条消息设置过期时间）上消息的存活时间，它的值是一个非负整数，单位为微秒。如果同时设置队列和队列中消息的<em>TTL</em>，则以较小的值为准。超过TTL的消息则成为<em>Dead Letter</em>（死信）。</p>
<p>死信可以重新路由到另一个<em>Exchange</em>（交换机），让消息重新被消费。通过设置<em>x-dead-letter-exchange</em>（<em>Dead Letter</em>重新路的交换机）和<em>x-dead-letter-routing-key</em>（转发的队列）来实现。<br>
<img src="https://yuncheng1998.github.io/post-images/1622514315383.png" alt="" loading="lazy"></p>
<p>发送消息时指定消息延迟的时间</p>
<pre><code class="language-java">public void send(String delayTimes) {
    amqpTemplate.convertAndSend(&quot;order.pay.exchange&quot;, &quot;order.pay.queue&quot;, &quot;延迟数据&quot;, message -&gt; {
        // 设置延迟毫秒值
        message.getMessageProperties().setExpiration(String.valueOf(delayTimes));
        return message;
    });
} 
</code></pre>
<p>设置延迟队列出现死信后的转发规则。</p>
<pre><code class="language-java">@Bean(name = &quot;order.delay.queue&quot;)
public Queue getMessageQueue() {
    return QueueBuilder
            .durable(RabbitConstant.DEAD_LETTER_QUEUE)
            // 配置到期后转发的交换
            .withArgument(&quot;x-dead-letter-exchange&quot;, &quot;order.close.exchange&quot;)
            // 配置到期后转发的路由键
            .withArgument(&quot;x-dead-letter-routing-key&quot;, &quot;order.close.queue&quot;)
            .build();
} 
</code></pre>
<h1 id="时间轮">时间轮</h1>
<p>https://www.cnblogs.com/luozhiyun/p/12075326.html</p>
<p>时间轮能够高效地利用线程资源来进行批量化调度，将任务绑定到同一个的调度器上面，使用这个调度器来进行任务的管理、触发以及运行。时间轮的结构如下所示<br>
<img src="https://yuncheng1998.github.io/post-images/1622514245143.png" alt="" loading="lazy"></p>
<p>左侧是一个存储定时任务的环形队列，底层采用数组实现，数组中的元素是一个存放定时任务的环形的双向链表，节点封装了真正的定时任务（<em>TimerTask</em>）。时间轮由多个时间格组成，每个时间格代表当前时间轮的基本时间跨度（<em>tickDuration</em>）。时间轮的时间格个数是固定的，可用 <em>wheel.length</em> 来表示。时间轮的表盘指针（<em>tick</em>）表示时间轮当前指针指向的<em>bucket</em>，此时处理对于列表中的所有任务。</p>
<h2 id="时间轮运行逻辑">时间轮运行逻辑</h2>
<p>时间轮在启动时记录当前启动的时间<em>startTime</em>，在添加任务时首先计算延迟时间（<em>deadline</em>），比如一个任务的延迟时间为<em>24ms</em>，任务将放在：当前的时间（<em>currentTime</em>）+<em>24ms</em>-<em>startTime</em> 所在的<em>bucket</em>的队列中。</p>
<p>时间轮运行后，会遍历<em>tick</em>指向的<em>TimerTask</em>列表，计算如下参数：</p>
<ol>
<li><strong><em>TimerTask</em>的总共延迟的次数</strong>：将每个任务的延迟时间（<em>deadline</em>）/<em>tickDuration</em> 计算出<em>tick</em>需要总共跳动的次数。</li>
<li><strong>时间轮<em>round</em>次数</strong>：(tick走的总次数-当前tick数量) / 时间格个数。比如<em>tickDuration</em>为<em>1ms</em>，时间格个数为<em>20</em>个，那么时间轮走一圈需要<em>20ms</em>，如果要添加进一个延时为<em>24ms</em>的数据，如果当前的tick为0，那么计算出的轮数为1，当指针运行第一圈时将<em>round</em>减一，运行第二圈才可以将轮数<em>round</em>减为0并会运行。</li>
<li>任务需要放置到时间轮的槽位，放入到槽位链表最后。</li>
</ol>
<p>参考：<a href="https://www.cnblogs.com/luozhiyun/p/12075326.html">时间轮算法（TimingWheel）是如何实现的？</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[顺序处理]]></title>
        <id>https://yuncheng1998.github.io/post/7xRdbGS0J/</id>
        <link href="https://yuncheng1998.github.io/post/7xRdbGS0J/">
        </link>
        <updated>2021-05-30T01:56:15.000Z</updated>
        <content type="html"><![CDATA[<p>绝大多数的场景下，业务操作不需要保证严格的顺序处理，但在数据存储上却是最常规的要求，比如MySQL在集群模式下多节点间的数据写入顺序必然是需要一致的。在业务操作上比较典型的是数据库日志的同步，我们一般会订阅到Kafka，然后从Kafka异步消费，这之中就要保证消费时记录的顺序与数据库一致。</p>
<p>顺序处理的基础是要做到时钟一致，本质的技术有以下几种：</p>
<p><strong>单节点处理</strong>：用一个节点处理所有消息，这种最简单，但有违微服务避免单点的原则，在可用性和可维护性上需要平衡，对一些边缘业务可采用此做法。</p>
<p><strong>单节时序生成</strong>：用一个节点生成<em>Timestamp</em>，这样就有了一个全局可排序的数据记录，当然也同样有违避免单点的原则，但这却是很多分布式数据库的选择（比如<em>TiDB</em>），因为它足够简单。</p>
<p><strong><em>TureTime</em>方案</strong>：由多个部署有<em>GPS</em>同步能力的时钟及原子钟节点提供<em>Timestamp</em>服务，这一方案避免了单点问题，问题在于太过昂贵，一般只有大型集群才会考虑使用（如<em>Google</em>的<em>Spanner</em>，使用这一方案保证不同服务节点的时间误差小于<em>10ns</em>）</p>
<p><em><strong>Lamport Timestamps</strong></em>：上面说的都是物理时钟，而<em>Lamport</em>提出的是逻辑时钟概念，通过为每一操作带上本地或接收到消息的时间戳来解决访问链路的顺序问题。<em>Lamport Timestamps</em>的局限只能处理有相关性记录的顺序，像上文说到数据库日志记录就无能为力了。</p>
<p>上述方案可以解决时钟一致性，但这远远不够。我们用<em>MQ</em>做服务解耦就会经常遇到顺序问题，比如将用户的关键操作流程通过<em>Kafka</em>传送到日志服务，日志就要保证顺序写入。但目前<em>Kafka</em>及主流的<em>MQ</em>都无法保证严格顺序，因为成本太高：要先保证生产者都同步生产消息到<em>MQ</em>，<em>MQ</em>的存储要避免多个写入节点，并且消费者只能有一个，进行逐条消费等，这在性能、可用性上都大打折扣。这时我们只能根据用户<em>Id</em>计算<em>hash</em>后分配到相同的写入节点（对应于<em>Kafka</em>的<em>Partition</em>），这样就能做到同一用户的日志消费顺序等同于日志的发送（同步发送方式）顺序。</p>
<p>顺序处理的成本不低，在开发中我们应该尽量避免。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[分布式锁]]></title>
        <id>https://yuncheng1998.github.io/post/FpNwKynsD/</id>
        <link href="https://yuncheng1998.github.io/post/FpNwKynsD/">
        </link>
        <updated>2021-05-28T14:56:51.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>如非必要切勿用锁。一方面锁会将并行逻辑转成串行严重影响性能，另一方面还要考虑锁的容错，处理不当可能导致死锁。</p>
</blockquote>
<h1 id="分布式锁的替代方案">分布式锁的替代方案</h1>
<h2 id="set化后的mq替代分布式锁">Set化后的MQ替代分布式锁</h2>
<p>可以按用户ID做Set（用户ID % Set数）进而分成多个组，为不同的组创建不同的MQ队列，这样一个用户同一时间只在一个队列中，一个队列的处理是串行化的，实现了锁的功能，同时又有多个Set来完成并行化，在性能上会好于分布式锁。</p>
<h2 id="使用乐观锁">使用乐观锁</h2>
<p>创建一个更新版本字段（update_version）,每次更新时版本加1，更新的条件是版本号要等于传入版本号：</p>
<pre><code class="language-java">  var (balance,currentVersion) = db.account.getBalanceAndVersion(id)
  if(balance &lt; amount){
    return error(&quot;余额少于扣款金额&quot;)
  }
  // 此操作对应的SQL: UPDATE account SET balance = balance  - &lt;amount&gt; , update_verison = update_verison + 1 WHERE id = &lt;id&gt; AND update_version = &lt;currentVersion&gt;
  if(db.account.updateBalance(id, -amount, currentVersion) == 0){
  return error(&quot;扣款失败&quot;) // 或递归执行此代码进行重试
  }
</code></pre>
<h1 id="使用分布式锁需要注意的问题">使用分布式锁需要注意的问题</h1>
<h2 id="锁释放与超时">锁释放与超时</h2>
<p>在单机情况下，我们只需要在使用完锁后在finally代码中将其释放掉，但在分布式环境下由于网络是不可靠的，节点可能宕机，会导致锁无法释放，所以我们必须要设置超时时间。超时时间设置过长会导致服务异常后无法及时获取新的锁，过短又有可能在业务没有执行完锁提前释放了。优雅但复杂的方法是使用心跳超时设置，即与占有锁的服务保持心跳，在心跳超时后再释放锁。</p>
<h2 id="性能及高可用">性能及高可用</h2>
<p>出于性能考虑，一般分布式锁都是非公平锁，如果要保证加锁顺序而选用公平锁时要注意对性能的影响，加解锁操作本身要保证性能及可用性，避免单点，锁信息要持久化，慎用自旋避免CPU浪费。</p>
<blockquote>
<p>在激烈竞争的情况下，非公平锁的性能高于公平锁的性能的一个原因是：在恢复一个被挂起的线程与该线程真正开始运行之间存在着严重的延迟。假设线程 A 持有一个锁，并且线程 B 请求这个锁。由于这个锁已被线程 A 持有，因此 B 将被挂起。当 A 释放锁时，B 将被唤醒，因此会再次尝试获取锁。与此同时如果 C 也请求这个锁,那么 C 很可能会在 B 被完全唤醒之前获得、使用以及释放这个锁。这样 的情况是一种“双赢”的局面：B 获得锁的时刻并没有推迟，C 更早地获得了锁，并且吞吐量也获得了􏰀高。</p>
<p>当持有锁的时间相对较长，或者请求锁的平均时间间隔较长，那么应该使用公平锁。在这些情况下，”插队”带来的吞吐量提升(当锁处于可用状态时，线程却还处于被唤醒的过程中)则可能不会出现</p>
<p><em>—— 《Java并发编程实战》</em></p>
</blockquote>
<h2 id="数据一致性">数据一致性</h2>
<p>分布式锁要合理设置锁标记以区分是哪个实例、哪个线程的操作，可重入锁要做好计数及相应的unlock次数，同时必须保证数据的一致，这要求我们只能选择CP特性的服务作为分布式锁的中间件。</p>
<h1 id="主流的分布式锁">主流的分布式锁</h1>
<h2 id="关系型数据库">关系型数据库</h2>
<p>由关系型数据库的某些特性来实现，比如使用主键唯一性约束及数据一致来确保同一时间只有一个请求能获得锁，这一方案实现简单，但对高并发场景或可重入时存在比较大的性能瓶颈。</p>
<h2 id="redis">Redis</h2>
<p>可使用Redis单线程、原子化操作（<em>setnx</em>）来实现，这一方案也很简单，但因为没有原子化的值比较方法，无法原子化确认占用锁的是否是当前实例的当前线程导致比较难实现重入锁，另外Redis单节点有高可用问题，多节点引入RedLock也存在比较大的<a href="https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html">争议</a>。当然在绝大多数情况下大家还是可以放心使用的。</p>
<h2 id="zookeeper">Zookeeper</h2>
<p>可使用Zookeeper的持久节点（PERSISTENT）、临时节点（EPHEMERAL），时序节点（SEQUENTIAL ）的特性组合及watcher接口实现，这一方案可保证最为严格的数据一致性、在性能及高可用也有着比较好的表现，推荐对一致性高要求极高、并发量大的场景使用。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[分布式缓存的设计]]></title>
        <id>https://yuncheng1998.github.io/post/00KtfUfg0/</id>
        <link href="https://yuncheng1998.github.io/post/00KtfUfg0/">
        </link>
        <updated>2021-05-28T06:37:36.000Z</updated>
        <content type="html"><![CDATA[<p>相比本地缓存，分布式缓存在设计使用上需要注意以下几点：</p>
<h2 id="缓存失效">缓存失效</h2>
<p>在用缓存时不可避免地会涉及缓存失效，失效有多种原因，比如过期时间设置不合理大量Key同时过期，请求直接访问数据库进而导致数据库压力陡增，再比如批量重建缓存导致缓存雪崩。对这一问题有多种解决方案，就仅针对缓存本身而言需要错开各Key过期时间及分批次刷新</p>
<h2 id="缓存穿透">缓存穿透</h2>
<p>在缓存失效中更为普遍的是缓存穿透：请求一个数据库不存在的记录，程序上不去缓存这些不存在记录对应的key，进而导致每次请求都去查数据库，这是缓存设计最容易被遗漏的地方。一般情况下不会有问题，不会有大量这种“错误”的请求，但如果被攻击利用就可能是致命的，原本要保护数据库资源的缓存完全失效。知道原理后解决的方案也简单：无论请求条件在数据库中存不存在都加入到缓存即可。当然这样会带来两个问题：1）现在为空不代表以后一直为空，2）可能存在大量值为空的垃圾记录。第一个问题是关于缓存一致性，如果Key为查询条件，需要设置业务上比较合理的过期时间，如果key为Id，只要在持久化到数据库时更新缓存中Id对应的值即可，第二个问题多为攻击情况下会产生很多为空的记录，以Redis为例，常用操作（时间复杂度为O(1)）对Key的数量并不敏感，这些记录只会占用一定的空间，对性能影响有限，当然考虑存储的压力我们也可以用BloomFilter或Hyperloglog来避免这一问题，如果是IP路由则本地BloomFilter或Hyperloglog，反之也可以使用分布式方案，分布式BloomFilter可用如Redis的Orestes-Bloomfilter、Rebloom （以Redis模块的方式加载）、Redisson，分布式Hyperloglog为Redis自带的数据结构。缓存穿透还有一种特殊的场景：高并发访问某一新增的、未被缓存的记录，在第一条请求从数据库读取到加入缓存并生效的时间窗口内大量的并发会穿透缓存给数据库带来很大的压力，这也极有可能被用于攻击，解决的方法可以是限流，当然这种做法过于粗暴，限流应该是保障系统可用性的最后几道屏障，是不得以而为之的，可以是数据变更时先写缓存再写数据库，这可能会带来一致性风险，也可以是写完数据库后立刻更新缓存（不等到读取时再更新缓存），但并不是所有记录都需要缓存，这可能会导致大量不必要的缓存开销，还可以是对穿透到数据库查询的代码进行加锁，笔者认为这种方法最为优雅，实现的伪代码如下：</p>
<pre><code class="language-java">  /**
    * 假定有并发请求1、2、3
    * 请求1先进入synchronized代码块，请求2、3等待
    * 请求1会查询数据库并更新缓存，退出synchronized
    * 请求2、3依次进入synchronized并命中缓存
    */
  // 先从缓存读取对应id的信息流
  var record = cache.get(&quot;feed:&quot; + id)
  if (record == null) {
    // 缓存未命中时加锁，要求串行化执行
    synchronized {
      // 再次尝试从缓存读取对应id的信息流
      record = cache.get(&quot;feed:&quot; + id)
      if (record == null) {
        // 从数据库读取并更新缓存
        record = db.feed.get(id)
        cache.put(&quot;feed:&quot; + id, record)
      }
    }
  }
</code></pre>
<h2 id="缓存一致性">缓存一致性</h2>
<p>缓存与数据库的数据一致性视不同情况而定，大部分场景下都要求是一致的，即数据库数据变更要同步刷新到缓存中，但刷新缓存可能会是一个耗时、耗IO的操作，尤其是数据批量变更时缓存数据的延时不可忽视，另外存在多级缓存时也需要考虑缓存内的数据同步，在缓存设计时数据同步及一致性问题要重点考虑</p>
<h2 id="热点数据">热点数据</h2>
<p>现实场景中我们的数据不大可能做到均匀分布，必定会出现冷热分化，缓存用于存放热点数据，但有一些特殊情况下会出了极热点的数据，这些数据会对分布式缓存的个别节点造成很大的压力，如业务上存在这种场景需要考虑做多级缓存为这些极热点的数据添加前置缓存层或是将Key Hash化以规避数据倾斜</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[分布式事务]]></title>
        <id>https://yuncheng1998.github.io/post/kX9yjKh02/</id>
        <link href="https://yuncheng1998.github.io/post/kX9yjKh02/">
        </link>
        <updated>2021-05-28T06:22:11.000Z</updated>
        <content type="html"><![CDATA[<p>分布式环境下我们必须假定网络是不可靠，它可能中断、超时，消息可能乱序、丢包，所以分布式事务就是基于这个前提下设计的。常见的分布式事务有二阶段提交、补偿型事务和通知型事务。</p>
<h1 id="二阶段提交">二阶段提交</h1>
<p>二阶段提交（<em>Two-phase commit protocol</em>）简称2PC是最基础的分布式事务方案。它将事务分为提交请求和提交两个阶段，另外它还定义了两个角色：协调器（<em>coordinator</em>）、参与者（<em>cohorts</em>）</p>
<figure data-type="image" tabindex="1"><img src="https://yuncheng1998.github.io/post-images/1622183011309.png" alt="" loading="lazy"></figure>
<h2 id="提交请求阶段">提交请求阶段</h2>
<ol>
<li>协调器向所有参与者发送事务提交请求命令，并等待所有参与者的答复。</li>
<li>每个参与者执行收到的事务提交请求。</li>
<li>每个参与者执行事务成功则返回同意（<em>agreement</em>），反之返回中止（<em>abort</em>）。</li>
</ol>
<h2 id="提交阶段">提交阶段</h2>
<ol>
<li>如果协调器收到的都是同意，那么它：
<ol>
<li>向所有参与者发送提交（<em>commit</em>）命令。</li>
<li>每个参与者执行提交操作，释放上一步打开的本地锁和资源。</li>
<li>每个参与者返回确认（<em>acknowledgment</em>）。</li>
<li>协调器在收到所有确认后完成事务。</li>
</ol>
</li>
<li>如果协调器收到有包含中止命令，那么它：
<ol>
<li>向所有参与者发送回滚（<em>rollback</em>）命令。</li>
<li>每个参与者执行回滚操作，释放上一步打开的本地锁和资源。</li>
<li>每个参与者返回确认（<em>acknowledgment</em>）。</li>
<li>协调器在收到所有确认后回滚事务。</li>
</ol>
</li>
</ol>
<h2 id="分析">分析</h2>
<p>2PC是CP设计，势必会损失可用性，它的问题在于：</p>
<ul>
<li>同步阻塞，执行中所有参与者的事务都会阻塞，所占的锁及资源不会释放。</li>
<li>数据不一致，在<strong>提交阶段</strong>如果出现网络故障部分参与者可能会收不到提交命令从而导致数据不一致。</li>
<li>单点故障，作为事务处理重要组成的协调器存在单点问题。</li>
</ul>
<h1 id="补偿型事务">补偿型事务</h1>
<p>补偿性事务（<em>Try-Confirm-Cancel</em>）简称<em>TCC</em>，<em>Try</em> 对应于<em>2PC</em> 的提交请求阶段，<em>Confirm</em> 对应的提交阶段的成功处理，<em>Cancel</em> 对应的是提交阶段的回滚处理，但与<em>2PC</em> 本质的区别在于：<em>2PC</em> 是两个阶段只有一个事务，<strong>而<em>TCC</em> 分别对应了三个事务操作</strong>。</p>
<h2 id="try-阶段"><em>Try</em> 阶段</h2>
<p>完成业务检查及资源预处理，以订单支付为例，用户发起订单支付后对应冻结操作：</p>
<ol>
<li>资金服务在本地事务下冻结支付金额（<em>UPDATE account SET balance_freeze = balance_freeze+&lt;支付金额&gt; WHERE id = &lt;当前账户&gt;</em>）</li>
<li>优惠券服务在本地事务下冻结使用的优惠券（<em>UPDATE coupon SET status = 'FREEZE' WHERE id = &lt;使用的优惠券&gt;</em>）</li>
<li>积分服务在本地事务下冻结支付成功后奖励的积分（<em>UPDATE account SET points_freeze = points_freeze+&lt;奖励积分&gt; WHERE id = &lt;当前账户&gt;</em>）</li>
</ol>
<h2 id="confirm-阶段"><em>Confirm</em> 阶段</h2>
<p>确认并执行业务，执行只涉及<em>Try</em> 阶段预处理的资源：</p>
<ol>
<li>资金服务在本地事务下解冻支付金额并完成实际扣款（<em>UPDATE account SET balance_freeze = balance_freeze-&lt;支付金额&gt; , balance = balance+&lt;支付金额&gt; WHERE id = &lt;当前账户&gt;</em>）</li>
<li>优惠券服务在本地事务下解冻使用的优惠券并完成优惠券的使用（<em>UPDATE coupon SET status = USED WHERE id = &lt;使用的优惠券&gt;</em>）</li>
<li>积分服务在本地事务下解冻积分并完成积分奖励（<em>UPDATE account SET points_freeze = points_freeze-&lt;奖励积分&gt; , points = points+&lt;奖励积分&gt;  WHERE id = &lt;当前账户&gt;</em>）</li>
</ol>
<h2 id="cancel-阶段"><em>Cancel</em> 阶段</h2>
<p>取消执行的业务，释放<em>Try</em> 阶段预留的资源，如果出现余额不足、优惠券不可用等情况则执行回滚操作，执行<em>Try</em> 的逆向操作，使最终结果看上去没有发生过一样，如对应的：</p>
<ol>
<li>积分服务在本地事务下解冻积分（<em>UPDATE account SET points_freeze = points_freeze-&lt;奖励积分&gt; WHERE id = &lt;当前账户&gt;</em>）</li>
<li>优惠券服务在本地事务下解冻使用的优惠券（<em>UPDATE coupon SET status = UNUSED WHERE id = &lt;使用的优惠券&gt;</em>）</li>
<li>资金服务在本地事务下解冻支付金额（<em>UPDATE account SET balance_freeze = balance_freeze-&lt;支付金额&gt; WHERE id = &lt;当前账户&gt;</em>）</li>
</ol>
<h2 id="分析-2">分析</h2>
<p>相比于2PC中所有请求提交阶段占用的资源都在等待提交阶段释放，两个阶段之间需要等待所有参与者响应，所以花费的时间会比较久，TCC不存在全局长事务，它将一个大事务分成三个阶段，每个阶段的每个实例都有一个个独立的本地事务，每个本地事务都各自提交，只在需要的时候回滚，所以TCC有着比2PC更高的性能。</p>
<p>但事务补偿对业务的侵入比较大，一次事务需要涉及3个阶段的代码编写，一方面提高了开发维护的成本，另一方面它也不适用于无法自己主导的工程，比如与三方服务之间的事务处理。</p>
<h1 id="通知型事务">通知型事务</h1>
<p>通知型事务为了解决2PC性能问题及TCC业务侵入问题，它将事务看作消息，使用MQ来传递，分为可靠通知和最大努力通知。</p>
<h2 id="最大努力通知">最大努力通知</h2>
<p>最大努力通知的前提是服务存在依赖，上游成功后下游要求必定成功，如果失败会有一定的策略重试，如果重试策略还失败，上游服务需要提供一个查询的接口以便获取上游服务事务成功后的数据。这一方案最直接的使用场景是跨系统间的数据处理，比如业务系统与支付网关，在支付网关支付成功后意味着钱已经流转，网关会通知（异步回调）作为下游的业务系统，一次失败后再会重试几次，如果再失败就需要业务系统主动来网关查询处理结果，而这过程中要求下游回调接口必须幂等。</p>
<h2 id="可靠通知">可靠通知</h2>
<p>可以理解为支持回滚的最大努通知，在重试策略也失败后可靠通知会执行事务回滚，这样一来就没有服务依赖及必须成功的约束，反之服务需要提供事务回滚逻辑，对业务有少许侵入。</p>
<h1 id="分布式事务设计原则">分布式事务设计原则</h1>
<p>无论是哪种分布式事务，都需要类似有事务协调器这一服务，都需要确保以下几个内容：</p>
<ul>
<li>事务协调器与各参与者（业务服务）内的调用必须幂等，即重试不会导致数据异常。</li>
<li>事务协调器与各参与者（业务服务）内的调用必须有超时时间，不能无限或长时间地等待。</li>
<li>事务必须能确保发送到事务协调器，这是大前提，对于MQ而言可使用AMQP规范的实现，如RabbitMQ，开启AMQP的事务机制</li>
</ul>
<p>我们回看分布式事务下的ACID保证，原子性（Atomicity）和持久性（Durability）与传统事务无异，但一致性（Consistency）与隔离性（Isolation）上除了2PC、3PC完全满足外不同实现的补偿型与通知型事务都有或多或少的缺失，它们都强调最终一致性，即允许在一段可接受的时间内各节点数据不一致，由于它们多半是将大事务分解成一个个本地小事务，所以在一段时间也存在隔离性问题。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[幂等]]></title>
        <id>https://yuncheng1998.github.io/post/RPVoJEAnp/</id>
        <link href="https://yuncheng1998.github.io/post/RPVoJEAnp/">
        </link>
        <updated>2021-05-28T03:36:10.000Z</updated>
        <content type="html"><![CDATA[<h1 id="何为幂等">何为幂等</h1>
<p>在程序中如果相同条件下多次请求对资源的影响表现一致则称请求为幂等请求，对应的接口为幂等接口。</p>
<p>在分布式环境下强调幂等是由于对通信链路的不信任，请求可能由于网络问题而出错进而需要做重试，如果接口没有做请求去重就可能导致重复处理引发数据错误，所以在分布式系统中接口的幂等性非常重要。</p>
<h1 id="幂等的解决方案">幂等的解决方案</h1>
<p>数据库主键或唯一约束：实现简单，但通用性欠缺，仅能应用于依赖数据库的业务，并且性能上需要权衡。</p>
<p>分布式锁：一般适用于需要长时间处理的任务，如数据导出、复杂计算等，在处理期间防止重复请求，由于这些操作本身就要求串行处理，所以加锁对性能地影响有限（锁粒度为请求条件）。</p>
<p>有限状态机：很多情况下请求对象都是带状态的，并且状态的跃迁是单向的，如订单的状态多为 <em>已下单-待付款-已付款-待发货-已发货-待签收</em> ，那么对于发货请求只能是针对已下单，待付款的订单，对其状态进行判断即可实现去重。</p>
<h2 id="通用解决方案">通用解决方案</h2>
<p>通用的做法是<strong>请求执行业务前先判断请求是否存在</strong>，如果存在则返回错误，否则写入请求并执行业务处理，处理完成后更新请求的状态并返回业务处理结果，如果业务处理错误可以选择删除对应的请求以便进行重试。该流程要求请求必须有可区分是否是重试的标识，并且对业务处理的前置判断和后置更新最好在框架层面实现以对业务操作透明化，做到最小化的业务逻辑侵入。</p>
<p>我们可以借助Redis实现（如果要记录的资源量很大时可以考虑使用BloomFilter，注意精度问题），要求请求方做一定的策略，用Redis记录请求Token，请求Token是请求方为同一请求（包含重试）生成唯一凭证用于判断是否是重复请求。</p>
<p>主流的MQ实现在 <code>autocommit=true</code> 时天然实现了幂等，但考虑业务处理可能出错的情况我们一般会将autocommit设置成false，在业务处理成功后再提交，这时就需要使用上述幂等方案了：在接收到消息时写入请求Token以实现去重判断（Token可为Topic+Offset）提交后删除Token，整体上可以做到对业务透明。</p>
<h2 id="幂等的传递性">幂等的传递性</h2>
<p>当幂等的业务操作只有部分成功时，应该如何处理？最直接的方法是将多个步骤用事务包裹，要么全部成功，要么全部失败，失败后删除请求<em>Token</em>允许重试。但微服务化后很有可能涉及到对不同服务的调用，此时我们只有两个选择：</p>
<p>1）引入分布式事务，分布式事务相对较重，如非必须建议慎用。</p>
<p>2）要求被调用的接口也实现幂等，这种做法相对优雅，也比较推荐。</p>
<p>因此幂等是有传递性的，它要求我们请求对应的接口及其接口调用的子孙接口都必须幂等，这也是幂等在微服务下实施的难点，所以使用一套对业务透明的通用幂等方案非常有必要。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CP与AP的取舍]]></title>
        <id>https://yuncheng1998.github.io/post/bzWkeZxM-/</id>
        <link href="https://yuncheng1998.github.io/post/bzWkeZxM-/">
        </link>
        <updated>2021-05-27T16:37:14.000Z</updated>
        <content type="html"><![CDATA[<h1 id="cap理论">CAP理论</h1>
<p>分布式系统设计要考虑三个核心要素：一致性（<em>Consistency</em>）、可用性（<em>Availability</em>）和分区容错性（<em>Partition tolerance</em>），而这三个特性不可能同时满足。</p>
<ul>
<li><strong>一致性</strong>：同一时刻同一请求的不同实例返回的结果相同。</li>
<li><strong>可用性</strong>：所有读写请求在一定时间内得到正确的响应。</li>
<li><strong>分区容错性</strong>：在网络异常情况下，系统仍能正常运作。</li>
</ul>
<p>由于分布式环境下网络的故障是常态，比如光缆被挖断、专线故障、网络波动、丢包、节点宕机等，所以设计时要考虑的是在满足P的前提下选择C还是A。</p>
<p>以分布式缓存系统为例，如果要保证一致性（C），就要求一个节点写入数据后必须再将数据写入到另一个节点后才能返回成功，当发生网络异常两个服务节点无法相互通讯，为保证一致性必须抛出异常进而牺牲了可用性（A）。如果要保证可用性（A），就要求两个节点数据同步前也必须可用，这就必须牺牲一致性（C）导致在一段时间内两个服务节点所查询到的数据可能不同。</p>
<h1 id="base理论">Base理论</h1>
<p>Base是基本可用（<em>Basically Available</em>）、软状态（<em>Soft State</em>）和最终一致性（<em>Eventual consistency</em>）三个短语的简写。</p>
<p>大量的工程实践的经验表明可用性很重要，一致性也很重要，但可以容许一定的时差，只要保证在一定时间内达到一致即可，这也就是<strong>最终一致性</strong>。因为要实现强一致性的成本很高，尤其是存在很多数据副本的情况下，区块链的PoW及其衍生算法的共识机制是概率强一致性（<em>Probabilistic Strong Consistency</em>），要求大多数节点都接受了这笔交易再真正接受它，这就导致交易的确认严重滞后。</p>
<p>Base方案允许系统在一段时间内存在数据不一致性，存在软状态，但在规定的时间后数据会最终一致性。这样做的好处在于满足可用性的同时也在一定程度上符合一致性的要求。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kafka入门（三）消费者]]></title>
        <id>https://yuncheng1998.github.io/post/lcHc6iPLj/</id>
        <link href="https://yuncheng1998.github.io/post/lcHc6iPLj/">
        </link>
        <updated>2021-05-18T07:31:21.000Z</updated>
        <content type="html"><![CDATA[<p>应用程序使用<em>KafkaConsumer</em> 订阅主题并接收这些主题的消息，然后把消息保存起来。</p>
<p>如果生产者对该主题的写入速度很快，单个消费者跟不上消息生成的速度，这时就需要多个消费者共同参与消费，对消息进行分流处理。消费者从属于消费者群组。<strong>一个群组中的消费者订阅的都是相同的主题</strong>，每个消费者接收主题一部分分区的消息。</p>
<p>向群组中增加消费者是<strong>横向伸缩消费能力</strong>的主要方式，创建主题时使用比较多的分区数可以在消费负载高的情况下增加消费者来提升性能。消费者的数量如果大于分区数多，那么会有消费者是空闲的，没有任何帮助。</p>
<p><em>Kafka</em> 一个很重要的特性是：只需写入一次消息，就可以支持任意多的应用读取这个消息。假如新增了一个有两个消费者的消费组，那么就演变为下图这样。<br>
<img src="https://yuncheng1998.github.io/post-images/1621323205486.png" alt="" loading="lazy"></p>
<p>此时两个消费组都能收到<em>T1</em> 主题的全量消息，在逻辑意义上来说它们属于不同的应用。</p>
<p>总结：如果应用需要读取全量消息，那么请为该应用设置一个消费组；如果该应用消费能力不足，那么可以在这个消费组里增加消费者。</p>
<h1 id="消费者组和分区重平衡">消费者组和分区重平衡</h1>
<h2 id="消费者组是什么">消费者组是什么</h2>
<p>消费者组*（Consumer Group）<em>是由一个或多个消费者实例</em>（Consumer Instance）<em>组成的群组，具有可扩展性和可容错性。组内的消费者共享一个消费者组ID</em>（Group ID）*，对一个主题进行订阅和消费，同一组中的消费者只能消费一个分区的消息，多余的消费者会闲置，派不上用场。</p>
<p>两种消费方式</p>
<ul>
<li>点对点的消费方式：一个消费者群组消费一个主题中的消息。</li>
<li>发布-订阅模式：一个主题中的消息被多个消费者群组共同消费。</li>
</ul>
<h2 id="消费者重平衡">消费者重平衡</h2>
<p>重平衡：这种把分区的所有权通过一个消费者转到其他消费者的行为。</p>
<figure data-type="image" tabindex="1"><img src="https://yuncheng1998.github.io/post-images/1621323266224.png" alt="" loading="lazy"></figure>
<p>当新增或减少消费者时，且消费者的数量小于分区数量时，就会触发重平衡。</p>
<p>优点：为消费者群组带来了<strong>高可用性</strong>和<strong>伸缩性</strong>，用户可以放心的添加消费者或移除消费者。</p>
<p>缺点：在重平衡期间，消费者无法读取消息，造成整个消费者组在重平衡的期间都不可用。当分区被重新分配给新消费者时，消息当前的读取状态会丢失，它有可能还需要去刷新缓存，在它重新恢复状态之前会拖慢应用程序。</p>
<h1 id="创建消费者">创建消费者</h1>
<h2 id="订阅主题">订阅主题</h2>
<p><em>subscribe()</em> 方法接受一个主题列表作为参数，使用起来比较简单</p>
<pre><code class="language-java">Properties properties = new Properties();
properties.put(&quot;bootstrap.server&quot;,&quot;192.168.1.9:9092&quot;);     properties.put(&quot;key.serializer&quot;,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;);   properties.put(&quot;value.serializer&quot;,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;);
KafkaConsumer&lt;String,String&gt; consumer = new KafkaConsumer&lt;&gt;(properties);
// 订阅主题，参数是一个正则表达式
consumer.subscribe(Arrays.asList(&quot;customerTopic&quot;));
</code></pre>
<h2 id="轮询">轮询</h2>
<p><em>Kafka</em> 支持订阅/发布模式的，生产者发送数据给<em>Broker</em>，消费者采用轮询的方式定期去<em>Broker</em> 中进行数据的检索</p>
<pre><code class="language-java">try {
  // 轮询
  while (true) {
    // 循环请求数据（心跳）
    // records包含记录所属主题、分区、消息在分区中的偏移量以及键值对
    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofSeconds(100));
    // 逐条处理每条记录
    for (ConsumerRecord&lt;String, String&gt; record : records) {
      log.debug(record.topic() + record.partition() + record.offset() + record.key() + record.value());
      // 处理record
      // ...
    }
  }
} finally {
  // 使用close()方法关闭消费者，会立即触发一次重平衡，而不是等待群组协调器发现它不再发送心跳并认定它已经死亡。
  consumer.close();
}
</code></pre>
<p><strong>线程安全性</strong></p>
<p>在同一个群组无法让一个线程运行多个消费者，也无法让多个线程安全共享一个消费者。按照规则，一个消费者使用一个线程，如果一个消费者群组中多个消费者都要运行，必须让每个消费者在自己的线程中运行，可以使用<em>Java</em> 中的<em>ExecutorService</em> 启动多个消费者进行进行处理。</p>
<h1 id="参数配置">参数配置</h1>
<p><em><strong>fetch.min.bytes</strong></em></p>
<p>设置消费者从服务器获取记录的最小字节数。<em>broker</em> 在收到消费者的数据请求时，如果可用的数据量小于<em>fetch.min.bytes</em> 指定的大小，会等到有足够的可用数据时才把它返回给消费者。这样在主题使用频率低时不需要处理消息，降低消费者和<em>broker</em> 的工作负载。如果没有很多可用数据，但消费者的 CPU 使用率很高，那么就需要把该属性的值设得比默认值大。如果消费者的数量比较多，把该属性的值调大可以降低 broker 的工作负载。</p>
<p><em><strong>fetch.max.wait.ms</strong></em></p>
<p>设置消息从<em>broker</em>发送到<em>Consumer</em> 的最长时间，默认<em>500ms</em>。当要打包消息的大小不满足<em>fetch.min.bytes</em> 时，则等待<em>fetch.max.wait.ms</em> 之后，不管满足不满足，将这个批次的消息发送给消费者。如果要降低潜在的延迟，就可以把参数值设置的小一些。如果<em>fetch.max.wait.ms</em> 被设置为 <em>100ms</em>，<em>fetch.min.bytes</em> 的值设置为<em>1MB</em>，那么<em>Kafka</em> 在收到消费者请求后，要么返回<em>1MB</em> 的数据，要么在<em>100ms</em> 后返回所有可用的数据。</p>
<p><em><strong>max.partition.fetch.bytes</strong></em></p>
<p>设置服务器从每个分区里返回给消费者的最大字节数，默认值为<em>1MB</em>。<em>KafkaConsumer.poll()</em> 方法从每个分区获得的记录不超过 <em>max.partition.fetch.bytes</em> 指定的字节。在默认值下，如果某主题有20个分区和5个消费者，那么每个消费者需要至少4MB的可用内存来接收记录（防止某个消费者崩溃，剩下的消费者处理更多分区）。另外该值需要大于<em>broker</em> 能够接收的最大消息的字节数*（max.message.size）*，否则消费者可能无法读取这些消息，导致消费者一直挂起重试。</p>
<p>在设置该属性需要考虑消费者处理数据的时间。消费者需要频繁的调用<em>poll()</em> 方法来避免会话过期和发生分区再平衡，如果返回的数据太多，消费者处理时间长，可能无法及时进行下一个轮询来避免会话过期。这时需要调大值或者延长会话过期时间。</p>
<p><em><strong>session.timeout.ms</strong></em></p>
<p>设置消费者在被认为死亡之前可以与服务器断开连接的时间，默认是 <em>3s</em>。认定为死亡后，协调器就会触发重平衡。把它的分区分配给消费者群组中的其它消费者。把值设置的比默认值小，可以更快地检测和恢复崩溃的节点，不过长时间的轮询或垃圾收集可能导致非预期的重平衡。把该属性的值设置得大一些，可以减少意外的重平衡，不过检测节点崩溃需要更长的时间。</p>
<p><em><strong>heartbeat.interval.ms</strong></em></p>
<p>设置<em>poll()</em> 方法向群组协调器发送心跳的频率，一般和<em>session.timeout.ms</em>同时修改，<em>heartbeat.interval.ms</em>必须比<em>session.timeout.ms</em> 小，一般为后者的三分之一。</p>
<p><em><strong>auto.offset.reset</strong></em></p>
<p>设置消费者在读取一个没有偏移量的分区或者偏移量无效的情况下的该如何处理。它的默认值是<em>latest</em>：在偏移量无效的情况下，消费者将从最新的记录开始读取数据。另一个值是<em>earliest</em>：在偏移量无效的情况下，消费者将从起始位置处开始读取分区的记录。</p>
<p><em><strong>enable.auto.commit</strong></em></p>
<p>设置消费者是否自动提交偏移量，默认值是 <em>true</em>，为了尽量避免出现重复数据和数据丢失，可以设置为 <em>false</em>，由自己控制何时提交偏移量。如果把它设置为 true，还可以通过 <strong>auto.commit.interval.ms</strong> 属性来控制提交的频率</p>
<p><em><strong>partition.assignment.strategy</strong></em></p>
<p>我们知道，分区会分配给群组中的消费者。<code>PartitionAssignor</code> 会根据给定的消费者和主题，决定哪些分区应该被分配给哪个消费者，Kafka 有两个默认的分配策略<code>Range</code> 和 <code>RoundRobin</code></p>
<p><em><strong>client.id</strong></em></p>
<p>该属性可以是任意字符串，broker 用他来标识从客户端发送过来的消息，通常被用在日志、度量指标和配额中</p>
<p><em><strong>max.poll.records</strong></em></p>
<p>设置单次调用 *call()*方法能够返回的记录数量，控制在轮询中需要处理的数据量。</p>
<p><em><strong>receive.buffer.bytes 和 send.buffer.bytes</strong></em></p>
<p>设置<em>socket</em> 在读写数据时用到的 <em>TCP</em> 缓冲区大小。如果它们被设置为 -1，就使用操作系统默认值。如果生产者或消费者与 <em>broker</em> 处于不同的数据中心内，由于跨数据中心的网络一般都有比较高的延迟和比较低的带宽，因此可以适当增大这些值。</p>
<h1 id="偏移量">偏移量</h1>
<p>消费者在每次调<em>poll()</em> 方法进行定时轮询的时候，会返回由生产者写入<em>Kafka</em> 但是还没有被消费者消费的记录，因此我们需要追踪哪些记录是被群组里的哪个消费者读取的。</p>
<p>消费者会向一个叫*_consumer_offset* 的特殊主题中发送消息，这个主题保存每次所发送消息中的分区偏移量，用于消费者触发重平衡后记录偏移使用的。当触发重平衡后，每个消费者可能会分到新的分区，这个主题就是让消费者能够继续处理消息。</p>
<ol>
<li>如果提交的偏移量小于客户端最后一次处理的偏移量，那么位于两个偏移量之间的消息就会被重复处理。</li>
<li>如果提交的偏移量大于最后一次消费时的偏移量，那么处于两个偏移量中间的消息将会丢失</li>
</ol>
<h2 id="提交偏移量的方式">提交偏移量的方式</h2>
<h3 id="自动提交">自动提交</h3>
<p><em>enable.auto.commit=true</em>时，消费者会自动把从<em>poll()</em> 方法轮询到的最大偏移量提交上去。提交时间间隔由<em>auto.commit.interval.ms</em> 控制，默认是 5s。自动提交是在轮询中进行的，消费者在每次轮询中会检查是否提交该偏移量了，如果是，那么就会提交从上一次轮询中返回的偏移量。</p>
<h3 id="同步提交">同步提交</h3>
<p><em>auto.commit.offset=false</em> 时可以自行设置应用程序提交偏移量的时间。使用<em>commitSync()</em> 会提交由<em>poll()</em> 方法返回的最新偏移量，提交成功后马上返回，失败就抛出异常。处理完所有记录后要确保调用了<em>commitSync(</em>)，否则还是会有丢失消息的风险，如果发生了重平衡，从最近一批消息到发生重平衡之间的所有消息都将被重复处理。</p>
<h3 id="异步提交">异步提交</h3>
<p>与同步提交<em>commitSync()</em> 最大的区别在于异步提交不会进行重试。</p>
<h3 id="同步和异步组合提交">同步和异步组合提交</h3>
<p>一般情况下，提交失败是因为临时问题导致的，后续自动重试提交会成功。但如果在关闭消费者或重平衡前的最后一次提交，就要确保提交成功。因此，在消费者关闭之前一般会组合使用<em>commitAsync</em>和<em>commitSync</em>提交偏移量。</p>
<h3 id="提交特定的偏移量">提交特定的偏移量</h3>
<p>消费者API允许调用 <em>commitSync()</em> 和 <em>commitAsync()</em> 方法时传入希望提交的 <em>partition</em> 和 <em>offset</em> 的 <em>map</em>，即提交特定的偏移量。</p>
]]></content>
    </entry>
</feed>